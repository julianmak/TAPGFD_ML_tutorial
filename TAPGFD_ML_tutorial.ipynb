{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I61ukIwyj0uD"
   },
   "source": [
    "* 12 May 2024, Julian Mak (some tidy up of conda usage)\n",
    "* 30 Apr 2022, Fei Er Yan + Julian Mak (whatever with copyright, do what you want with this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lkzLYJpPyWA",
    "outputId": "110d6a37-9d28-4402-b495-77c501e8241a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE (JM 20 May): various things that could fix, some fixes below, (un)comment as required\n",
    "\n",
    "# A) if using Colab, will need a separate upload of the data and mount on colab\n",
    "#\n",
    "# 1) go to https://drive.google.com/drive/folders/1JJ0cpshu6-JE8wp93UsHuqy6V33rQy7s?usp=sharing\n",
    "# 2) download the folder\n",
    "# 3) upload that to your own instance of Colab\n",
    "# 4) load as something like below:\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#data = xr.open_zarr(\"/path/to/folder/GLOB_HOMOGENEOUS_variables.zarr/\")\n",
    "\n",
    "# test to see if data actually exists (should return some folders)\n",
    "#ls \"/content/drive/MyDrive/Colab Notebooks/GLOB_HOMOGENEOUS_variables.zarr/\" including BRV2\n",
    "\n",
    "# B) engine problems, e.g. unrecognized engine zarr must be one of: ['h5netcdf', 'scipy', 'store']\n",
    "#\n",
    "# brute force fix is to install complete\n",
    "# !pip install xarray[complete] # xarray[io] # zarr\n",
    "#\n",
    "# then need to force a kernel restart (\"Runtime -> Restart Session\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "# loose introduction to Machine learning\n",
    "\n",
    "[Machine Learning](https://en.wikipedia.org/wiki/Machine_learning) has found a ton of applications in multiple disciplines, notably in:\n",
    "\n",
    "* image and facial recognition\n",
    "* natural language processing (e.g. predictive text, translations)\n",
    "* classical literature (e.g. [Kuzushiji](https://nips2018creativity.github.io/doc/deep_learning_for_classical_japanese_literature.pdf))\n",
    "* self-driving cars (e.g. [HKUST efforts](https://seng.hkust.edu.hk/news/20200402/autonomous-vehicles-developed-hkust-engineering-professor-serve-community-during-covid-19-outbreak-mainland-china))\n",
    "* Artificial Intelligence (e.g. the famous [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo))\n",
    "\n",
    "There has also been applications in multiple aspects of environmental sciences, including oceanography. In this notebook we are going to briefly touch on the more elementary applications. The focus here will be on demonstrating some use of machine learning and the relevant Python syntax, and less on the understanding of the algorithms themselves (they can get quite technical and mathematical). The explanations do exist, but we will largely use them like a *black box*, and just assume we have made some deal/sacrifice with say [Hermaeus Mora](https://elderscrolls.fandom.com/wiki/Hermaeus_Mora) in exchange for some answers.\n",
    "\n",
    "<img src=\"https://i.imgur.com/ZFxVaBm.jpg\" width=\"400\" alt='Hermeowus Mora'>\n",
    "\n",
    "(Disciple of Hermaeus Mora, Hermeowus Mora)\n",
    "\n",
    "> NOTE: Again, these algorithm are just tools, and they have their own limitations, and should not be regarded as silver bullets that will solve all your problems. One criticism I have with these things is that these tools can work really well, but you might not understand why they work so well, and that is something that generally makes me a little uneasy.\n",
    "\n",
    "Machine learning can briefly be split into **unsupervised** and **supervised** learning. Unsupervised is when you let the algorithms find features for you, while supervised is when the data itself is already tagged, and a model is *trained up* to try and reproduce the target data.\n",
    "\n",
    "PCA would be an example of unsupervised learning, where you feed in the data, and the algorithm returns features that capture certain amounts of variance. Linear regression would be an example of supervised learning I guess, where given some outputs and inputs, you want to find some sort of model that minimises the mismatch between outputs and predictions using inputs. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "## a) Argo data\n",
    "\n",
    "For our demonstration here we are going to be using data from the [argo observation system](https://argo.ucsd.edu/). Argo is a system of autonomous floats that are put into the ocean, floating around with the currents, and periodically does vertical sections to take in-situ measurements of things like temperature, salinity, pressure, and so forth down to about 2000 m depth; see below for the schematic. There are increasing interest in [BGC-Argo](https://biogeochemical-argo.org/) that measure quantities relevant to biogeochemistry, and [deep Argo](https://argo.ucsd.edu/expansion/deep-argo-mission/) that go down to 4000 m.\n",
    "\n",
    "<img src=\"https://argo.ucsd.edu/wp-content/uploads/sites/361/2020/06/float_cycle_1-768x424.png\" width=\"600\" alt='Argo'>\n",
    "\n",
    "> NOTE: The namesake of argo is related to the [JASON](https://en.wikipedia.org/wiki/Jason-1) satellites if you know your Greek mythology.\n",
    "\n",
    "There are multiple products and file formats that one could get (see later). Instead of the more standard **gridded** reanalyses products, we are going to be dealing with the float profile data directly. The data is collated in the folder `GLOB_HOMOGENEOUS_variables.zarr`, which can be opened with the [`zarr`](https://zarr.readthedocs.io/en/stable/) plugin, a so far experimental data format that promises to give better performance particularly when parallel processing is concerned. In this case we happen to open it using the `zarr` plugin through `xarray`.\n",
    "\n",
    "> NOTE: If you are on your own computer, you probably want to do `conda install -c conda-forge zarr` in your environment. It might be available already on Google Colab, but if not, try `!pip install xarray[io]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data (good luck...)\n",
    "#\n",
    "#data = xr.open_zarr(\"/path/to/folder/GLOB_HOMOGENEOUS_variables.zarr/\")\n",
    "\n",
    "data = xr.open_zarr(\"./GLOB_HOMOGENEOUS_variables.zarr/\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x0nqmIc1l8E"
   },
   "source": [
    "The files themselves exists as separate binary files, which is then collated into an xarray object here. The data is in dimensions of **depth** and **profile number**. There are various variables, but we are only going to be using `PSAL` (practical salinity) and `TEMP` (*in-situ* temperature). These are subsetted out in the code below, with a little bit of tidying up:\n",
    "\n",
    "1) dropping all the entries with salinity that are outside an expected range (here anything outside of the 25 to 40 g/kg interval)\n",
    "\n",
    "2) profiles that have a NaN entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0L3YXKbxjCq"
   },
   "outputs": [],
   "source": [
    "# subset some data out\n",
    "\n",
    "da_al = data[['PSAL','TEMP']] #(DEPTH: 302, N_PROF: 128910)\n",
    "da_s  = da_al.where((da_al.PSAL <40.) & (da_al.PSAL>25.), drop= True)\n",
    "\n",
    "# NOTE (JM Apr 15): if it complains about no indexing by booleans, try adding the .compute() bit in as below\n",
    "# da_s  = da_al.where(((da_al.PSAL <40.) & (da_al.PSAL>25.)).compute(), drop= True)\n",
    "\n",
    "da_s  = da_s.dropna('N_PROF')\n",
    "da_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the raw profile data as a function of geographical co-ordinates\n",
    "\n",
    "As per tradition, we follow the number -1 step of data analysis and plot out what the data looks like.\n",
    "\n",
    "Each profile has associated with it a longitude and latitude, so in the below we do a scatter plot, and each profile is marked on as a dot. We colour the dots by the magnitude of the data, so this ends up looking a bit like a contour/pcolor graph.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> You can try and make plots through Cartopy in the stuff below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "Rvh43bn2AZno",
    "outputId": "fa3eeba1-d3e6-4257-e9d5-b16ffe75f7cc"
   },
   "outputs": [],
   "source": [
    "# plot out what the observation data actually looks like\n",
    "\n",
    "nl = 20 # change this index to plot different depths (as an index entry)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "\n",
    "# temperature\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cs = ax.scatter(da_s.LONGITUDE, da_s.LATITUDE, 10, da_s.TEMP[:,nl], \n",
    "                cmap=plt.cm.get_cmap('Spectral_r'), zorder=3)\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "plt.colorbar(cs)\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.set_title(f\"Temp at {da_s.DEPTH[nl].values} m\")\n",
    "\n",
    "# salinity\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "cs = ax.scatter(da_s.LONGITUDE, da_s.LATITUDE, 2, da_s.PSAL[:,nl], \n",
    "                alpha=.5, cmap=plt.cm.get_cmap('viridis', 5), zorder=3)\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "plt.colorbar(cs)\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.set_title(f\"Salinity at {da_s.DEPTH[nl].values} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AGVR98ZwaR1"
   },
   "source": [
    "We consider subsetting the data a bit more into different regions. Below are some of the ones we chose in light of the clustering analysis we will be doing later, using xarray conditionals. We then plot out the data to see the geographical distribution of the data we subset out at a fixed depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDs6hcC-OYqf"
   },
   "outputs": [],
   "source": [
    "# subsetting and showing different locations\n",
    "\n",
    "# North Atlantic\n",
    "da_na=da_s.where(  (da_s['LATITUDE']  >   0.) \n",
    "                 & (da_s['LATITUDE']  <= 50.) \n",
    "                 & (da_s['LONGITUDE'] > -78.) \n",
    "                 & (da_s['LONGITUDE'] <  31.),\n",
    "                 drop=True)\n",
    "\n",
    "# higher latitude, split into Atlantic and Pacific sector\n",
    "da_ao=da_s.where(da_s['LATITUDE'] > 50., drop=True)\n",
    "da_ao1=da_s.where((da_s['LATITUDE'] > 50.) & (da_s['LONGITUDE'] >-78.) & (da_s['LONGITUDE'] < 31), drop=True)\n",
    "da_ao2=da_ao.where((da_ao['LONGITUDE'] >= 100.) | (da_ao['LONGITUDE'] <= -100), drop=True)\n",
    "\n",
    "# Southern Ocean, split into a few sectors\n",
    "da_so=da_s.where(da_s['LATITUDE'] <=  -56., drop=True)\n",
    "da_so1=da_s.where((da_s['LATITUDE'] > -56.) & (da_s['LATITUDE'] <= -40.) , drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "FnBlwFfX9IPi",
    "outputId": "f22f7ec5-0313-4614-8d82-7e13dd6357ef"
   },
   "outputs": [],
   "source": [
    "# plot out the locations that have been subsetted out\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes()\n",
    "ax.plot(da_s.LONGITUDE  , da_s.LATITUDE,   \"o\", markersize=2, label=\"total\")\n",
    "ax.plot(da_na.LONGITUDE , da_na.LATITUDE,  \"o\", markersize=2, label='North Atlantic')\n",
    "ax.plot(da_ao1.LONGITUDE, da_ao1.LATITUDE, \"o\", markersize=2, label='Arctic Ocean')\n",
    "ax.plot(da_so.LONGITUDE , da_so.LATITUDE,  \"o\", markersize=2, label='Southern Ocean')\n",
    "ax.plot(da_so1.LONGITUDE, da_so1.LATITUDE, \"o\", markersize=2, label='Southern Ocean 1')\n",
    "ax.plot(da_ao2.LONGITUDE, da_ao2.LATITUDE, \"o\", markersize=2, label='Arctic Ocean 2')\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "plt.grid()\n",
    "ax.legend()\n",
    "ax.set_title(f\"Geographical locations of subsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the raw data in $TS$-space\n",
    "\n",
    "The diagram below show the above labelled data in a **$TS$-diagram** (i.e. data but in temperature-salinity space) at some chosen depths, which provides another visualisation of the data that is perhaps more in line with the **watermass properties**. Notice for example the North Atlantic data tends to be clustered in a certain region in $TS$-space, with the distinguishing feature of it being generally quite salty; this is consistent with the observations that the Atlantic waters tend to be more salty because of the known physical oceanic processes at play (e.g. lec 5 of OCES 2003; `09/10_fun_with_maps` data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "94q8JF_bM9kL",
    "outputId": "c10d387d-ca1e-45b9-d121-fc46aea56f07"
   },
   "outputs": [],
   "source": [
    "# TS-diagrams at different depths\n",
    "nl = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(da_s.PSAL  [:, nl], da_s.TEMP  [:, nl], \"o\", markersize=2, label=\"total\")\n",
    "ax.plot(da_na.PSAL [:, nl], da_na.TEMP [:, nl], \"o\", markersize=2, label='North Atlantic')\n",
    "ax.plot(da_ao1.PSAL[:, nl], da_ao1.TEMP[:, nl], \"o\", markersize=2, label='Arctic Ocean')\n",
    "ax.plot(da_so.PSAL [:, nl], da_so.TEMP [:, nl], \"o\", markersize=2, label='Southern Ocean')\n",
    "ax.plot(da_so1.PSAL[:, nl], da_so1.TEMP[:, nl], \"o\", markersize=2, label='Southern Ocean 1')\n",
    "ax.plot(da_ao2.PSAL[:, nl], da_ao2.TEMP[:, nl], \"o\", markersize=2, label='Arctic Ocean 2')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "ax.set_ylabel(r'Temperature ($^\\circ\\ \\mathrm{C}$)')\n",
    "ax.set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "ax.set_title(f\"TS diagram at {da_s.DEPTH[nl].values} m\")\n",
    "\n",
    "nl = 20\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(da_s.PSAL  [:, nl], da_s.TEMP  [:, nl]  , \"o\", markersize=2, label=\"total\")\n",
    "ax.plot(da_na.PSAL [:, nl], da_na.TEMP [:, nl] , \"o\", markersize=2, label='North Atlantic')\n",
    "ax.plot(da_ao1.PSAL[:, nl], da_ao1.TEMP[:, nl], \"o\", markersize=2, label='Arctic Ocean')\n",
    "ax.plot(da_so.PSAL [:, nl], da_so.TEMP [:, nl] , \"o\", markersize=2, label='Southern Ocean')\n",
    "ax.plot(da_so1.PSAL[:, nl], da_so1.TEMP[:, nl], \"o\", markersize=2, label='Southern Ocean 1')\n",
    "ax.plot(da_ao2.PSAL[:, nl], da_ao2.TEMP[:, nl], \"o\", markersize=2, label='Arctic Ocean 2')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "ax.set_ylabel(r'Temperature ($^\\circ\\ \\mathrm{C}$)')\n",
    "ax.set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "ax.set_title(f\"TS diagram at {da_s.DEPTH[nl].values} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the data as meridional sections\n",
    "\n",
    "For demonstration purposes we are going to focus on the Atlantic here. We also plot out the raw profile data as a scatter plot as above to show the distribution of temperature and salinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFLENpd_KYxh"
   },
   "outputs": [],
   "source": [
    "# select Atlantic sector\n",
    "\n",
    "da_aw = da_s.where((da_s['LONGITUDE'] > -75.) & (da_s['LONGITUDE'] < 17), drop=True)\n",
    "da_aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "xT26LEoHKiyj",
    "outputId": "d1828b85-9176-419d-b2a2-db3d58dabead"
   },
   "outputs": [],
   "source": [
    "# plot temperature and salnity at fixed depth only in Atlantic sector\n",
    "\n",
    "nl = -20\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cs = ax.scatter(da_aw.LONGITUDE, da_aw.LATITUDE, 10, da_aw.TEMP[:, nl],\n",
    "                cmap = plt.cm.get_cmap('Spectral_r', 10))\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "ax.set_title(f\"Temperature at {da_s.DEPTH[nl].values} m\")\n",
    "plt.colorbar(cs)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "cs = ax.scatter(da_aw.LONGITUDE, da_aw.LATITUDE, 10, da_aw.PSAL[:, nl],\n",
    "                cmap = plt.cm.get_cmap('viridis', 10))\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "ax.set_title(f\"Salinity at {da_s.DEPTH[nl].values} m\")\n",
    "plt.colorbar(cs)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data here is not gridded and each profile has its own longitude and latitude, it is not immediately possible to do meridional sections, and further processing is required. \n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> One way is to **interpolate** the data. This could be done through e.g. `scipy.interpolate`, since you basically have a collection of co-ordinates with associated data, or through xarray (which as far as I can tell leverages the `scipy.interpolate` anyway). The other leverages the xarray functionality `.groupby('LATITUDE')`, and then taking averages (skipping NaN values). This procedure results in an averaging over profiles, longitude and time, so is really a meridional section of the time and zonally averaged data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2y7jphmfyCw"
   },
   "source": [
    "---------------------------\n",
    "\n",
    "## b) Example of unsupervised learning: cluster analysis\n",
    "\n",
    "As advertised above, unsupervised learning is where you let the algorithms pick out the data features of interest. PCA (and by corollary EOF analysis) is one example of this, which we have encountered already in `04_regression`. These kind of algorithms have found uses in image recognition and reconstruction; the example below shows PCA applied to pictures of cats and dogs (right panels are the first 4 PCs of data; from Fig 10 of [Brunton, Brunton, Proctor & Kutz (2013)](https://arxiv.org/pdf/1310.4217.pdf)) encountered in `04_regression`.\n",
    "\n",
    "<img src=\"https://i.imgur.com/D5TJanm.png\" width=\"800\" alt='brunton_et_al_13_fig10'>\n",
    "\n",
    "Unsupervised learning is useful for data exploration. In the argo data of interest to us here, we know from theory and observations already that different water masses clusters in a different way, so can machine learning pick those out for us? In the case below, we are going demonstrate the use of the **$k$-means** algorithm, which is one possible way of identifying clusters. $k$-means very loosely considers partitions of the data, computing the means of the data associated with each partition, and iterating on the choice of partition such that there is a minimisation of the deviations of the partitioned data from the means. The algorithm is available in `scikit-learn`, with syntax demonstrated below.\n",
    "\n",
    "> NOTE: Sounds familiar? A lot of machine learning could (and should?) be framed as an optimisation problem where we want to minimise some cost functional (often called the **loss function** in machine learning), and because it is an optimisation problem, we have some liberty in choosing the choice of mismatch and/or regularisations, which may help with finding \"better\" solutions depending on the context.\n",
    "\n",
    "For the case here we want to tell the algorithm what are the features of interest. We are going to stack the temperature and salinity data together, so the clustering tries to find clusters given both the temperature and salinity as an input feature.\n",
    "\n",
    "> NOTE: We are going to get rid of the deeper parts of the data as a choice of pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7jdsQxhf0IX"
   },
   "outputs": [],
   "source": [
    "# only demonstrating one, could try others (uncomment accordingly)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# from sklearn.cluster import DBSCAN\n",
    "# from sklearn.cluster import OPTICS\n",
    "\n",
    "fx = np.stack((da_s.TEMP[:, :-30].values, da_s.PSAL[:, :-30].values), axis=2)\n",
    "fx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, I am going to fix a depth level indexed by `nl` and consider a clustering with `nc` features. The model is the fitted with the input data (with a seed specified to make sure the initial guess is fixed and so results are exactly reproducible), and a prediction is made. We then plot it out the resulting clusters' distribution geographically as well as in $TS$-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_A10jhGAf41L"
   },
   "outputs": [],
   "source": [
    "# Fit to clustering model according to temp and salinity characteristics\n",
    "nc = 5  # number of features\n",
    "nl = 20 # level of data to be used\n",
    "\n",
    "seed = 3315088937\n",
    "model = KMeans(n_clusters=nc, random_state=seed)\n",
    "# model = GaussianMixture(n_components=nc, random_state=seed)\n",
    "# model = DBSCAN(eps=0.3, min_samples=1000)\n",
    "# model = OPTICS(min_samples=100)\n",
    "\n",
    "# fit data and use clustering for prediction\n",
    "model.fit(fx[:, nl])\n",
    "cluster_idx = model.predict(fx[:, nl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "euyAnWjwnQyz",
    "outputId": "fb0c08c8-175c-4a10-80dd-594d20c214e9"
   },
   "outputs": [],
   "source": [
    "# plot out the predictions\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# horizontally varying\n",
    "ax = plt.subplot2grid((1, 3), (0, 0), colspan=2)\n",
    "cs = ax.scatter(da_s.LONGITUDE, da_s.LATITUDE, nc, cluster_idx, cmap = plt.cm.get_cmap('viridis', nc))\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "ax.set_title(f'Clusters at {da_s.DEPTH[nl].values} m')\n",
    "ax.grid()\n",
    "\n",
    "# on TS diagram\n",
    "ax = plt.subplot2grid((1, 3), (0, 2), colspan=1)\n",
    "cs = ax.scatter(fx[:, nl, 1], fx[:, nl, 0], nc, cluster_idx, cmap = plt.cm.get_cmap('viridis', nc))\n",
    "ax.set_ylabel(r'Temperature ($^\\circ\\ \\mathrm{C}$)')\n",
    "ax.set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "ax.set_title(f'Clusters on TS diagram at {da_s.DEPTH[nl].values} m')\n",
    "ax.grid()\n",
    "cax = plt.colorbar(cs)\n",
    "cax.set_ticks(np.arange(nc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the clusters we got here are not that similar to the manual ones we specified in the exploratory plots above. However, there are some physical rationalisations here:\n",
    "\n",
    "* the Eastern boundary water looks like it is indexed by cluster 0\n",
    "* the Southern Antarctic water is indexed by cluster 1, highlighting the water that are generally cold, and relatively fresh\n",
    "* there is a delineation between the Antarctic waters, between the colder and relatively fresh waters and what might be classified as the ACC waters that forms part of cluster 4\n",
    "* cluster 4 is picking out the polar waters\n",
    "* there is a suggestive pattern for the subpolar gyres given by cluster 2 and 3, which are generally warmer\n",
    "\n",
    "There have been some papers using similar techniques to identify watermass properties (e.g. [Jones et al., 2019](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018JC014629) for the Southern Ocean, using Gaussian Mixture Model).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> I haven't standardised the data here, but you should try and see if it makes a difference (hint: it does quite a bit). Remember to invert the transform if you are going to plot the data in a $TS$-diagram. See below in the neutral network part for some related code.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Look up and/or try the other clustering models that have been commented out above.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try doing the clustering analysis for smaller regions (e.g. the Southern Ocean region; cf. [Jones et al., 2019](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2018JC014629))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHbu9y36tdmC"
   },
   "source": [
    "---------------------------\n",
    "\n",
    "## c) Example of supervised learning: neural networks\n",
    "\n",
    "If we want to predict things we might want to employ (semi-)supervised learning instead. Ultimately we have\n",
    "\n",
    "\\begin{equation*}\n",
    "    Y = f(X),\n",
    "\\end{equation*}\n",
    "\n",
    "where $X$ is the input, $y$ is the output, and $f$ is the model. Generically, we talk about **training/fitting** a model $f$ via exposing the associated algorithm to some \n",
    "\n",
    "* **training data** ($X_{\\rm train}, Y_{\\rm train}$) to minimise misfits encapsulated in some **loss function** (usually some sort of square of the mismatch)\n",
    "\n",
    "* **validation data** ($X_{\\rm val}, Y_{\\rm val}$) for tuning model **hyperparameters** and/or selecting from a collection of models trained up\n",
    "\n",
    "The acid test for the performance of the trained model is then examined through\n",
    "\n",
    "* **test data** ($X_{\\rm test}, Y_{\\rm test}$) that the model has ***not*** seen before via some measure of mismatch between the \"truth\" data $Y_{\\rm test}$ and prediction $f(X_{\\rm test})$, with mismatch to be defined accordingly.\n",
    "\n",
    "(Multi-)Linear regression are examples of the more basic supervised learning algorithms in this regard, where for the $L^2$ misfit (or loss function) we don't strictly have to distinguish training, validation or test data (though we might consider having training and test data at least). More sophisticated nonlinear algorithms such as **neural networks** normally do want a splitting, and we will demonstrate the procedure below for the argo data.\n",
    "\n",
    "> NOTE: We are going to be using neural networks for supervised learning, but you could in principle use them for unsupervised as well as reinforcement learning. See for example the [wikipedia entry](https://en.wikipedia.org/wiki/Artificial_neural_network) on related procedures (you might want to do a Google search if you find the description on the Wikipedia page to abstract).\n",
    "\n",
    "The goal here is to ***predict salinity from temperature*** (more for demonstration rather than scientific reasons). In this case we will do a Z-score standardisation of all the data, and a minor tidy up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ywrq-5NKtvuk",
    "outputId": "7c802712-7e1a-4a73-deee-f814d91e40e6"
   },
   "outputs": [],
   "source": [
    "# standardise the data\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(da_s.PSAL)\n",
    "data_salt=scaler.transform(da_s.PSAL)\n",
    "\n",
    "scaler.fit(da_s.TEMP)\n",
    "data_temp=scaler.transform(da_s.TEMP)\n",
    "\n",
    "# input is temperature, output is salinity\n",
    "#   don't include the stuff at the lower depths\n",
    "xx = data_temp[:,:-30]\n",
    "yy = data_salt[:,:-30]\n",
    "\n",
    "# original size = (N_PROF: 128910, DEPTH: 302)\n",
    "print(f\"# of profiles = {xx.shape[0]} of size {xx.shape[1]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z54uDju8t7-b"
   },
   "source": [
    "We have over 100,000 profiles as a function of depth, and the input will be some vertical profile of temperature, while the output we want out of this is a vertical profile of salinity.\n",
    "\n",
    "In anticipation of demonstrating the neural network algorithm, we will in this case (somewhat randomly) divide the data up into a test, validation and training set, using the `train_test_split` sub-function from `scikit-learn`. The code below does the following:\n",
    "\n",
    "1) first split out 20% of the total data as a test set (so now we have 80% of the data left)\n",
    "\n",
    "2) we split out the remaining 25% of the data (25% of 80% = 20% of 100%) into a validation set\n",
    "\n",
    "3) what is left over (60% of the original data set) is the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMemrf8pcam-",
    "outputId": "4c488dda-2315-4cf1-a4a2-b08d37729d73"
   },
   "outputs": [],
   "source": [
    "# split into test data first, then validation, and the remaining are training data\n",
    "# X are the inputs, y are the outputs\n",
    "indices = np.arange(xx.shape[0])\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# split out test data (20% of 100%)\n",
    "X_tr, X_test, y_tr, y_test, x_ind, ind_test = train_test_split(xx, yy, indices,\n",
    "                                                               test_size=0.2, \n",
    "                                                               random_state=seed, \n",
    "                                                               shuffle=True)\n",
    "\n",
    "# from the remaining, split out the 80% data into 20% validation (hence the 0.25) and remaining to be training\n",
    "X_train, X_val, y_train, y_val, ind_train, ind_val = train_test_split(X_tr, y_tr, x_ind,\n",
    "                                                                      test_size=0.25, \n",
    "                                                                      random_state=seed, \n",
    "                                                                      shuffle=True)\n",
    "\n",
    "print(f\"number of test       data = {X_test.shape[0]}\")\n",
    "print(f\"number of validation data = {X_val.shape[0]}\")\n",
    "print(f\"number of training   data = {X_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the case here we will be using about 80% of the data to train up the neural network, and testing it against the 20% data that the network has not been previously exposed to. The exact splitting is flexible, but a 90:10 or 80:20 split is fairly common.\n",
    "\n",
    "> NOTE: For neural networks it is generally considered good to use as much training data as possible.\n",
    "\n",
    "The below graph plots the geographical distribution of the train/validation/test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "tgXW_hGbyadF",
    "outputId": "ceca6b25-6ec5-400d-cad0-894e04ba03b5"
   },
   "outputs": [],
   "source": [
    "# plot out where these are\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(14, 3), facecolor='w', edgecolor='k', sharey='row')\n",
    "\n",
    "X = da_s\n",
    "ax[0].plot(X.LONGITUDE.values[ind_train], X.LATITUDE.values[ind_train], 'C0.', markersize=0.5)\n",
    "ax[1].plot(X.LONGITUDE.values[ind_val],   X.LATITUDE.values[ind_val],   'C1.', markersize=0.5)\n",
    "ax[2].plot(X.LONGITUDE.values[ind_test],  X.LATITUDE.values[ind_test],  'C2.', markersize=0.5)\n",
    "\n",
    "ax[0].set_title(\"Train data (60%)\")\n",
    "ax[1].set_title(\"Validation data (20%)\")\n",
    "ax[2].set_title(\"Test data (20%)\")\n",
    "\n",
    "ax[0].set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax[0].set_ylabel(r\"lat ($^\\circ$)\")\n",
    "ax[1].set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax[2].set_xlabel(r\"lon ($^\\circ$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4oOYvJuaXfN"
   },
   "source": [
    "### First try: linear regression\n",
    "\n",
    "By the principle of Occam's razor we should probably at least try the simpler linear regression case first. We are going to use `scikit-learn` and train up a linear model using the (normalised) training dataset. We then plot the $L^2$ mismatch or the **root-mean-squared (RMS) loss** between $y_{\\rm test}$ and $f(X_{\\rm test})$. In this case I didn't bother undoing the scaling, so a RMS loss larger than 1 is pretty bad.\n",
    "\n",
    "> NOTE: Linear regression basically doesn't work, which is perhaps not a huge surprise, but lets demonstrate this explicitly. I am not going to be that careful about doing diagnostics for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqbAHDoea70n"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# do a linear regression (note data here has already by Z-scored)\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X_train, y_train)\n",
    "model.score(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the squared mismatches per index of prediction for SCALED data\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot((y_pred - y_test).flatten()**2, \"x\")\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.set_ylabel(r\"$(y - y_{\\mathrm{data}})^2$\")\n",
    "ax.grid()\n",
    "ax.set_title(r\"RMS mismatch for SCALED data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the array has been flattened, so each cross here is a prediction at some location and at some *fixed* depth. So over quite a few predictions has very large RMS loss values, indicating linear regression has failed pretty hard here. This is not entirely a surprise, given we do not expect there to be a linear relation between temperature and salinity, as can be seen from the $TS$-diagram. Increasing the number of inputs might help, so you could try this in the extended exercise later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JI7nutZouHFI"
   },
   "source": [
    "### Neural networks \n",
    "\n",
    "A neural network is a network with a schematic like the one below (diagram taken from [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)).\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/800px-Colored_neural_network.svg.png\" width=\"200\" alt='network schematic'>\n",
    "\n",
    "The idea here is that each node (the blobs) is some feature, and each link is a connection with some **weights** (which could be deterministic or probabilitistic in principle) leading to a transition, which is a recipe for transforming some input into some intermediate output. Given some input, the model splits it into multiple features, pass it through the network each with some transitional probabilities, and eventually leads to a collection of outputs that is assembled to give you an output. For a simple case where only the weights are varied, for each choice of weights there is an associated mismatch between the prediction and the provided \"truth\", and the goal is to iterate on the weights such that the eventual associated mismatch (or the loss function) is minimised, or at least approaches some asymptote. Again, it might be helpful to think of these as optimisation problems (which is certainly what I tend to do, because I am more familiar with optimisation problems).\n",
    "\n",
    "Depending on problem and available computational resources, various **hyperparameters** (e.g. the number of features, loss function decrease threshold, regularisation, number of hidden layers, model training parameters) might need/want to be varied. There are cases where one could vary the features themselves during the iterations, employ other algorithms (e.g. **convolution neural networks (CNN)**, **generative adversarial networks (GAN)** etc.), but this is well beyond the scope here.\n",
    "\n",
    "For our problem we are going to keep it simple and just use a standard neural network. We are going to be using the `keras` package, and the `tensorflow` backend (another possibility is `pytorch`).\n",
    "\n",
    "> NOTE: If you are using this notebook locally through Anaconda, you probably want to do `conda install -c conda-forge tensorflow` and `conda install -c conda-forge keras`. Tensorflow and keras should be available through Google Colab as is.\n",
    "\n",
    "> NOTE: The present problem is small so I am going to not bother with GPU capabilities. Look up the internet on how to get things working with GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDkxxS-igTJ7"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initialised here through `Sequential()`, and two hidden layers with 400 features are added in. The `Dropout` command is to drop some features (in this case 20% in each layer), which acts as a regulariser and reduces chances of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zx5nAwluSiAk",
    "outputId": "9999c58b-066d-4d3b-dafa-12f5001a530c"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(400, input_shape=(X_train.shape[1],), kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(400, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_test.shape[1], activation='linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the properties of neural network model, we proceed to train it with the testing data, and validate it using the validation data. Here we are training the model based on the RMS loss (specified via the `loss` keyword), with the optimisation done through the [adam](https://arxiv.org/abs/1412.6980) algorithm (which is an first order gradient based stochastic optimization). The model is going to train for 30 epochs (cf. full iterations).\n",
    "\n",
    "> NOTE: If the optional keyword `validation_data` is not specified I assume the model will just pick out some data from the provided training set to serve as validation data. It is specified here to force the model to be somewhat reproducible.\n",
    "\n",
    "> NOTE: \"adam\" is the name of the algorithm (it's not an acronym), and it's one of the go-to algorithms that is used in machine learning for the optimisation problem. Google scholar notes the adam paper ([Kingma & Ba, 2014](https://arxiv.org/abs/1412.6980)) has around 100,000 citations to date (checked in May 2022) and was the #1 most cited scientific paper of the past five years in 2020 [link](https://www.natureindex.com/news-blog/google-scholar-reveals-most-influential-papers-research-citations-twenty-twenty), so who says no one cares about numerical quantitative research..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMQHInUaWFyk",
    "outputId": "12e737ef-9b0a-4c11-f2be-fca26cbc54f5"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "batch_size = 1280\n",
    "epochs = 30\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training record is given in this case in the `history` variable. The code below plots out the RMS loss  against the epoch, and note that the loss is gradually decreasing, but is not zero. We probably don't want it zero, because that usually would indicate a model is very overfitted. Remember here the data is scaled and a RMS loss of 1 is pretty bad, so the model is getting a RMS loss below 0.1 for the whole dataset, which might be regarded as reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "lld-IZkRkxkX",
    "outputId": "caa504ec-f22b-4a66-997a-7e49508d387f"
   },
   "outputs": [],
   "source": [
    "# plot out diagnostic relevant training of neural network\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot(history.history['loss'])\n",
    "ax.plot(history.history['val_loss'])\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('model loss')\n",
    "ax.grid()\n",
    "ax.legend(['Train','validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byPWjGMwGOSi"
   },
   "source": [
    "Now that we have a model, we can proceed to use it. The acid test here is to use the model on the test data to see how the model performs, given the model has not been exposed to the test data at all. We just need to make sure to undo the data standardisation if we want a \"real\" output. The output in this case is salinity, and we make a subroutine below to undo the standardisation based on salinity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "1QDdFggrT5Pj"
   },
   "outputs": [],
   "source": [
    "# subroutine to return unscaled output\n",
    "\n",
    "def destd(x):\n",
    "    scaler1 = preprocessing.StandardScaler()\n",
    "    scaler1.fit(da_s.PSAL[:,:x.size])\n",
    "    return scaler1.inverse_transform(x.reshape(1,-1)).reshape(-1)\n",
    "\n",
    "# make prediction\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below randomly chooses three profiles in the test dataset and plots out the truth $y_{\\rm test}$ and the prediction $f(X_{\\rm test})$, and we should be able to see that the model is not perfect, but fairly reasonable in the deeper parts of the profile, with deficiencies in some cases near the ocean surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "B2UivbKkmXKa",
    "outputId": "6ba8362f-e4c3-4dbb-d274-b9d99debf171",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# randomly plot three profiles and the prediction from neural networks\n",
    "\n",
    "dp = da_s.DEPTH[:xx.shape[1]]\n",
    "np.random.seed(4167)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(14, 6), facecolor='w', edgecolor='k', sharey='row')\n",
    "\n",
    "for i in range(3):\n",
    "    ind = np.random.randint(y_pred.shape[0]+1)\n",
    "    ax[i].plot(destd(y_pred[ind]), dp, label='Prediction')\n",
    "    ax[i].plot(destd(y_test[ind]), dp, label='Test')\n",
    "    ax[i].set_title(f'Argo profile # {ind}')\n",
    "    ax[i].set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "    ax[i].grid()\n",
    "    \n",
    "ax[0].set_ylabel(r'Depth ($\\mathrm{m}$)')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXWyraj7GHsQ"
   },
   "source": [
    "To get a more quantitative measure, we compute the RMS loss of all profiles in the three datasets as a function of depth. Here we expect the model to perform reasonably well for the training and validation dataset, with the test data being the \"worse\", given the model has not seen the test data before. Just from seeing the above plots, we might have an expectation that the model would perform worse in terms of accuracy near the top of the ocean, and do better at depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "gnlN8spqcT60",
    "outputId": "1bf0f7b7-7691-446a-dcdf-66b7e1c742b8"
   },
   "outputs": [],
   "source": [
    "# compute the RMS errors between input data and prediction\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def finde(x_e, y_e):\n",
    "    y_pred = model.predict(x_e)       \n",
    "    return np.sqrt(mean_squared_error(y_pred, y_e, multioutput='raw_values')) \n",
    "  \n",
    "fig = plt.figure(figsize=(6, 8))\n",
    "ax = plt.axes()\n",
    "ax.plot(finde(X_test,  y_test),  dp, \"C3\", label=\"Test\") \n",
    "ax.plot(finde(X_train, y_train), dp, \"C0\", label=\"Train\")\n",
    "ax.plot(finde(X_val,   y_val),   dp, \"C1\", label=\"Validation\")\n",
    "ax.set_xlabel('RMS error')\n",
    "ax.set_ylabel(r'Depth ($\\mathrm{m}$)')\n",
    "ax.set_title('Averaged RMS error')\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHBO13FsF_FP"
   },
   "source": [
    "So the above plot is largely consistent with our expectations. It is however of interest to see that the RMS error starts increasing below 600 m depth. This could be a numerical or physical artifact, but we don't really have enough information to say thus far.\n",
    "\n",
    "> NOTE: The temperature here is the in-situ temperature rather than potential temperature, which might make a slight difference. The model behaviour could be arising from this particular realisation of the model training and can be tested by doing some sort of ensemble calculation to get the RMS loss (I do think the behaviour is probably generic, although I haven't looked). It could just be that the neural network is better at getting the bulk values rather than the extremes.\n",
    "\n",
    "Below we compile the histogram of RMS loss values over *depth*, with an aim to find geographical locations that the model performs particularly bad at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqMW_GPu8PAJ"
   },
   "outputs": [],
   "source": [
    "def finde1(x_e,y_e):\n",
    "    y_pred = model.predict(x_e)\n",
    "    err = np.zeros(y_e.shape[0])\n",
    "    for i in range(y_e.shape[0]):\n",
    "        err[i] = np.sqrt(((y_pred[i]-y_e[i])**2).mean())\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "fb4Ftburse_N",
    "outputId": "3ccb2596-1d3a-40cc-cf22-6458dd52cd87"
   },
   "outputs": [],
   "source": [
    "# histogram of error of SCALED data and associated predictions (>1 is bad basically) aveaged over depths\n",
    "\n",
    "eh_all = finde1(xx, yy)  # this is SCALED data\n",
    "eh1_a  = np.where(eh_all < 1, eh_all, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.axes()\n",
    "ax.hist(eh1_a, bins=25, color='#0504aa', alpha=0.7, rwidth=0.85)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('RMS')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biNbkd5nFxVd"
   },
   "source": [
    "From the histogram we see the model performs ok in most locations, with a slow-ish decaying tail as we get to the higher RMS losses. There are cases where the model does really badly though, as given the the RMS loss of 1 (again this is the RMS loss for scaled data, so 1 is bad).\n",
    "\n",
    "The plot below shows the geographical distribution of the RMS loss over depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "bRVmxeWNBF9l",
    "outputId": "cee3592d-69d0-4525-9788-19c4f94df745"
   },
   "outputs": [],
   "source": [
    "# plot out the geostrophica distribution of errors\n",
    "eh1_a = np.where(eh_all < 1, eh_all, 1)\n",
    "X = da_s\n",
    "\n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "\n",
    "# plot the raw errors\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cs = ax.scatter(X.LONGITUDE, X.LATITUDE, 2, eh1_a, cmap='cubehelix_r')\n",
    "ax.set_ylim(-70, 70)\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "plt.colorbar(cs)\n",
    "\n",
    "# make it a bit easier to read and limit the colorbars\n",
    "eh1_a = np.where(eh_all < 0.4, eh_all, 0.4)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "cs = ax.scatter(X.LONGITUDE, X.LATITUDE, 2, eh1_a, cmap='cubehelix_r')\n",
    "ax.set_ylim(-70, 70)\n",
    "ax.set_xlim(-180, 180)\n",
    "plt.colorbar(cs)\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sylitRsjs2L2"
   },
   "source": [
    "As we can see the places where the model seems to perform badly in the mediterranean sea and the outflow region, which is plausible since this is a region with particularly strong salinity (through high evaporation and low percipitation) that is perhaps not as well correlated with temperature. There are regions around the tip of South Africa and the Bay of Bengal in the Indian Ocean where the errors are also notable, which are also regions known to have salinity anomalies (the Aghulas rings transport salty water, while the Bay of Bengal is rather fresh by comparison).\n",
    "\n",
    "The below code picks out one of these cases with a larger RMS loss to compare the prediction and given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2bCgHV40w2ys",
    "outputId": "ee41f49e-eae8-4f82-c83d-e0d109887b9d"
   },
   "outputs": [],
   "source": [
    "lt_eh = eh_all.tolist()\n",
    "f_eh  = sorted(i for i in lt_eh if i >= 0.4)\n",
    "print(f\"number of profiles above error threshold = {len(f_eh)}\")\n",
    "print(f\"  totalling {len(f_eh)/xx.shape[0]*100:.2f}% of data\")\n",
    "\n",
    "y_predal = model.predict(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "Sfinw6qX5cJt",
    "outputId": "ed9b0d95-a6ec-4a64-bf4e-4ea065180293"
   },
   "outputs": [],
   "source": [
    "# randomly select one of the \"bad\" profiles and see what predictions look compared with data\n",
    "\n",
    "np.random.seed(4167)\n",
    "idx = lt_eh.index(f_eh[np.random.randint(len(f_eh)+1)])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2,  facecolor='w',figsize=(7,5), edgecolor='k', sharey='row')\n",
    "\n",
    "h1 = da_s.isel(N_PROF=idx)\n",
    "\n",
    "ax[0].plot(destd(y_predal[idx]), dp, label='Prediction')\n",
    "ax[0].plot(h1.PSAL.values,h1.DEPTH, '.-', label='Data')\n",
    "ax[0].set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "ax[0].set_ylabel(r'Depth ($\\mathrm{m}$)')\n",
    "ax[0].legend()\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot(h1.TEMP.values,h1.DEPTH,'.-', color='k')\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel('Temp ($^\\circ$)')\n",
    "\n",
    "fig.suptitle(f\"\"\"ARGO profile # {da_s.N_PROF[idx].values}\n",
    "[Lat: {h1.LATITUDE.values:05.2f}°, Lon: {h1.LONGITUDE.values:05.2f}°]\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLnyVTfW2YjH"
   },
   "source": [
    "For the chosen seed (4167), the model makes a fairly large error in the salinity throughout the depth (salinity unlike temperature doesn't vary that much in raw numerical values in the ocean). Without looking at other \"bad\" cases, we cannot definitively conclude here why the model fails at these particular cases (which is often an issue with black box models like neural networks...)\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Try picking out the particularly \"bad\" profiles (through xarray, error thresholds, or otherwise) and see what the model is actually doing there.\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> Consider doing ensemble type calculations to test for robustness of model behaviour and skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some personal comments\n",
    "\n",
    "I personally feel uneasy about black box models because you could be getting right and/or wrong answers for the wrong and/or right reasons, and you don't necessarily know why or have a good way to test it. It is my opinion that black box models need to be used with caution; the last thing I feel people should be doing is pressing buttons and relying blindly on the numbers that come out (too much doing not enough thinking). In the absence of knowing what the model is actually doing, it would be prudent to explore how and when the model works and/or fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) `argopy`\n",
    "\n",
    "Less machine learning and more data manipulation + argo data. There is now a package called [`argopy`](https://argopy.readthedocs.io/en/latest/) that you might want to try to use instead of the provided binary files here. Consider swapping out what we have done above that reads a local file for something that reads data off a remote database instead.\n",
    "\n",
    "> NOTE: Let me know if you end up doing this, as that will release some space on my GitHub LFS quota, and you can get acknowledgment on the code too :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Clustering algorithms for categorical data\n",
    "\n",
    "Try it for some other data (e.g. atmospheric data, biogeochemical data) and apply the clustering algorithms to see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Neural networks\n",
    "\n",
    "In our case we used temperature to predict salinity. Consider trying for example:\n",
    "\n",
    "* predict temperature from salinity\n",
    "\n",
    "* include other features such as depth and so forth\n",
    "\n",
    "* use temperature, salinity and or depth to predict `sigma0` (potential density referenced to sea surface); in this case we know the truth answer given `sigma0` is actually derived from temperature and salinity, so we could see if the neural network is at least able to reproduce the derived data itself\n",
    "\n",
    "* make up some for yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Random forests\n",
    "\n",
    "Have a look at the [random forest](https://en.wikipedia.org/wiki/Random_forest) algorithms and see what you get from that. \n",
    "\n",
    "Random forest might be more suited to the smaller dataset such as penguin or iris data (see [example](https://medium.com/edviconedu/random-forest-algorithm-to-classify-iris-flower-datasets-in-google-colab-b0652a8a8a66)). Try and do some of the exercises floated around these set of notebooks for some choice of datasets and questions of your own choosing (be your own teacher!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Project_pred_T_to_S_loc.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
